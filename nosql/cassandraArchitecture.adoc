ifndef::imagesdir[:imagesdir: ./images]
:toc:
= Casandra Architecture

== Data Centers and Racks

* used in systems spanning physically separate locations

A *rack* is a logical set of nodes in close proximity to each other, perhaps on physical machines in a single rack of equipment.  +
A *data center* is a logical set of racks, perhaps located in the same building and connected by reliable network.

Default configuration: a single data center ("datacenter1") containing a single rack ("rack1")

== Gossip and Failure Detection

Cassandra uses a gossip protocol that allows each node to keep track of state information about the other nodes in the cluster. The gossiper runs every second on a timer.

Gossip protocols generally assume a faulty network, are commonly employed in very large, decentralized network systems, and are often used as an automatic mechanism for replication in distributed databases.

* The gossip protocol in Cassandra is primarily implemented by the org.apache.cassandra.gms.Gossiper class, which is responsible for managing gossip for the local node.
* When a server node is started, it registers itself with the gossiper to receive endpoint state information.

* the Gossiper class maintains a list of nodes that are alive and dead

Steps for checking by the gossiper:

* Once per second, the gossiper will choose a random node in the cluster and initialize a gossip session with it. Each round of gossip requires three messages.

* The gossip initiator sends its chosen friend a *GossipDigestSyn* message.

* When the friend receives this message, it returns a *GossipDigestAck* message.

* When the initiator receives the ack message from the friend, it sends the friend a *GossipDigestAck2* message to complete the round of gossip.

*Phi Accrual Failure Detector*:

* decoupling it from the application being monitored
* a suspicion level

Failure detection is implemented in Cassandra by the org.apache.cassandra.gms.FailureDetector:

* `isAlive(InetAddressAndPort)`
What the detector will report about a given node’s alive-ness.

* `interpret(InetAddressAndPort)`
Used by the gossiper to help it decide whether a node is alive or not based on the suspicion level reached by calculating Phi (as described in the Hayashibara et al. paper).

* `report(InetAddressAndPort)`
When a node receives a heartbeat, it invokes this method.

== Snitches

* provide information about your network topology -> efficiently route requests

* determine relative host proximity for each node in a cluster

Read operation:

* it must contact a number of replicas determined by the consistency level
* selects a single replica to query for the full object, and asks additional replicas for hash values in order to ensure the latest version of the requested data is returned.
* The snitch helps to help identify the replica that will return the fastest, and this is the replica that is queried for the full data.

Default snitch (the SimpleSnitch) is topology unaware.

*Dynamic snitching*:

* helps optimize the routing of reads and writes over time.
* the selected snitch is wrapped with another snitch called the DynamicEndpointSnitch
* monitors the performance of requests to the other nodes

*Badness threshold*

* a configurable parameter that determines how much worse a preferred node must perform than the best-performing node in order to lose its preferential status.

== Rings and Tokens

* represents the data managed by a cluster as a ring
* Each node in the ring is assigned one or more ranges of data described by a token

* default configuration -> a token is a 64-bit integer ID used to identify each partition

* A node claims ownership of the range of values less than or equal to each token and greater than the last token of the previous node, known as a token range.

image::rings.png[Rings]

* Data is assigned to nodes by using a *hash function to calculate a token for the partition key*. This partition key token is compared to the token values for the various nodes to identify the range, and therefore the node, that owns the data.

CQL language provides a `token()` function - request the value of the token corresponding to a partition key

----
SELECT last_name, first_name, token(last_name) FROM user;

----

== Virtual Nodes

Instead of assigning a single token to a node, the token range is broken up into multiple smaller ranges.

* Each physical node is then assigned multiple tokens.
* each node has been assigned 256 of these tokens, meaning that it represents 256 virtual nodes
* number of vnodes by setting the`num_tokens` property in the cassandra.yaml

== Partitioners

A partitioner determines how data is distributed across the nodes in the cluster.

* organizes rows in partitions.
* Each row has a partition key
* A partitioner is a hash function for computing the token of a partition key.
* compute the token based on the partition key columns

* Any clustering columns that may be present in the primary key are used to determine the ordering of rows within a given node

The Murmur3Partitioner was added in 1.2 and has been the default partitioner since then

image::partitioner.png[partitioner]

== Replication Strategies

A node serves as a replica for different ranges of data.

* replication factor is the number of nodes in your cluster that will receive copies (replicas) of the same data

The first replica will always be the node that claims the range in which the token falls, but the remainder of the replicas are placed according to the replication strategy

*SimpleStrategy* places replicas at consecutive nodes around the ring, starting with the node indicated by the partitioner

*NetworkTopologyStrategy* allows you to specify a different replication factor for each data center. Within a data center, it allocates replicas to different racks in order to maximize availability.

== Consistency Levels

* For *read queries* -> the consistency level specifies how many replica nodes must respond to a read request before returning the data.
* For *write operations* -> the consistency level specifies how many replica nodes must respond for the write to be reported as successful to the client.

Consistency levels:

* *ONE, TWO, and THREE* - an absolute number of replica nodes that must respond to a request

* *QUORUM* - requires a response from a majority of the replica nodes

Q=floor(replication factor/2+1)

RF=3 -> Q=2, RF=4 -> Q=3, RF=5 -> Q=3

* *ALL* - requires a response from all of the replicas

The replication factor is set per keyspace. The consistency level is specified per query, by the client.

R(read replica count) + W(write replica count) > RF = strong consistency

_Recommended way to achieve strong consistency in Cassandra is to write and read using the QUORUM or LOCAL_QUORUM consistency levels_

== Queries and Coordinator Nodes

image::coordinatorNodes.png[Coordinator Nodes]

A client may connect to any node in the cluster to initiate a read or write query -> This becomes *the coordinator node*.

* The coordinator identifies which nodes are replicas for the data that is being written or read and forwards the queries to them.

* The coordinator node contacts all replicas, as determined by the consistency level and replication factor

== Hinted Handoff

A write request is sent to Cassandra, but a replica node where the write properly belongs is not available due to network partition, hardware failure, or some other reason.

* If the replica node where the write belongs has failed, the coordinator will create a hint, which is a small reminder
* once it detects via gossip that node B is back online, node A will “hand off” to node B the “hint” regarding the write.

Cassandra holds a separate hint for each partition that is to be written.

* In general, hints do not count as writes for the purposes of consistency level.
* The exception is the consistency level ANY - a hinted handoff alone will count as sufficient toward the success of a write operation.
 ** The write is considered durable, but the data may not be readable until the hint is delivered to the target replica.


When the other nodes notice that the failed node has come back online, they tend to flood that node with requests -> Cassandra limits the storage of hints to a configurable time window. It is also possible to disable hinted handoff entirely.

== Anti-Entropy, Repair, and Merkle Trees

Anti-entropy protocols are a type of gossip protocol for repairing replicated data.

Modes replica synchronization: *read repair and anti-entropy repair*.

Read repair:

* refers to the synchronization of replicas as data is read.
* Cassandra reads data from multiple replicas in order to achieve the requested consistency level, and detects if any replicas have out-of-date values.
* If an insufficient number of nodes have the latest value, a read repair is performed immediately to update the out-of-date replicas.
* Otherwise, the repairs can be performed in the background after the read returns.

Anti-entropy repair (manual repair):

* is a manually initiated operation performed on nodes as part of a regular maintenance process.
* is executed by using a tool called nodetool
* Running nodetool repair causes Cassandra to execute a *validation compaction*
* During a validation compaction, the server initiates a TreeRequest/TreeReponse conversation to exchange *Merkle trees* with neighboring replicas

_The Merkle tree is a hash representing the data in that table._

* If the trees from the different nodes don’t match, they have to be reconciled (or “repaired”) to determine the latest data values they should all be set to.

[NOTE]
====
Each table has its own Merkle tree. +
The tree is created as a snapshot during a validation compaction. +
It is kept only as long as is required to send it to the neighboring nodes on the ring
====

== Lightweight Transactions and Paxos

*Linearizable consistency* - to guarantee that no other client can come in between our read and write queries with their own modification

Cassandra supports a *lightweight transaction (LWT)* mechanism that provides linearizable consistency.

Cassandra’s LWT implementation is based on Paxos.

=== Paxos

*Paxos is a consensus algorithm* that allows distributed peer nodes to agree on a proposal, without requiring a leader to coordinate a transaction

Basic Paxos algorithm consists of two stages:

* prepare/promise
* propose/accept

To modify data:

* a coordinator node can propose a new value to the replica nodes, taking on the role of leader.

IMPORTANT:  Other nodes may act as leaders simultaneously for other modifications

* Each replica node checks the proposal, and if the proposal is the latest it has seen, it promises to not accept proposals associated with any prior proposals

* Each replica node also returns the last proposal it received that is still in progress.

* If the proposal is approved by a majority of replicas, the leader commits the proposal, but with the caveat that it must first commit any in-progress proposals that preceded its own proposal

The Cassandra implementation extends the basic Paxos algorithm to support the desired read-before-write semantics (also known as check-and-set), and to allow the state to be reset between transactions

* Prepare/Promise
* Read/Results
* Propose/Accept
* Commit/Ack

NOTE: A successful transaction requires four round-trips between the coordinator node and replicas

* Cassandra’s lightweight transactions are limited to a single partition

== Memtables, SSTables, and Commit Logs

Cassandra stores data both in memory and on disk to provide both high performance and durability.

image::storageEngine.png[Storage engine]