ifndef::imagesdir[:imagesdir: ./images]
:toc:
= Casandra Architecture

== Data Centers and Racks

* used in systems spanning physically separate locations

A *rack* is a logical set of nodes in close proximity to each other, perhaps on physical machines in a single rack of equipment.  +
A *data center* is a logical set of racks, perhaps located in the same building and connected by reliable network.

Default configuration: a single data center ("datacenter1") containing a single rack ("rack1")

== Gossip and Failure Detection

Cassandra uses a gossip protocol that allows each node to keep track of state information about the other nodes in the cluster. The gossiper runs every second on a timer.

Gossip protocols generally assume a faulty network, are commonly employed in very large, decentralized network systems, and are often used as an automatic mechanism for replication in distributed databases.

* The gossip protocol in Cassandra is primarily implemented by the org.apache.cassandra.gms.Gossiper class, which is responsible for managing gossip for the local node.
* When a server node is started, it registers itself with the gossiper to receive endpoint state information.

* the Gossiper class maintains a list of nodes that are alive and dead

Steps for checking by the gossiper:

* Once per second, the gossiper will choose a random node in the cluster and initialize a gossip session with it. Each round of gossip requires three messages.

* The gossip initiator sends its chosen friend a *GossipDigestSyn* message.

* When the friend receives this message, it returns a *GossipDigestAck* message.

* When the initiator receives the ack message from the friend, it sends the friend a *GossipDigestAck2* message to complete the round of gossip.

*Phi Accrual Failure Detector*:

* decoupling it from the application being monitored
* a suspicion level

Failure detection is implemented in Cassandra by the org.apache.cassandra.gms.FailureDetector:

* `isAlive(InetAddressAndPort)`
What the detector will report about a given node’s alive-ness.

* `interpret(InetAddressAndPort)`
Used by the gossiper to help it decide whether a node is alive or not based on the suspicion level reached by calculating Phi (as described in the Hayashibara et al. paper).

* `report(InetAddressAndPort)`
When a node receives a heartbeat, it invokes this method.

== Snitches

* provide information about your network topology -> efficiently route requests

* determine relative host proximity for each node in a cluster

Read operation:

* it must contact a number of replicas determined by the consistency level
* selects a single replica to query for the full object, and asks additional replicas for hash values in order to ensure the latest version of the requested data is returned.
* The snitch helps to help identify the replica that will return the fastest, and this is the replica that is queried for the full data.

Default snitch (the SimpleSnitch) is topology unaware.

*Dynamic snitching*:

* helps optimize the routing of reads and writes over time.
* the selected snitch is wrapped with another snitch called the DynamicEndpointSnitch
* monitors the performance of requests to the other nodes

*Badness threshold*

* a configurable parameter that determines how much worse a preferred node must perform than the best-performing node in order to lose its preferential status.

== Rings and Tokens

* represents the data managed by a cluster as a ring
* Each node in the ring is assigned one or more ranges of data described by a token

* default configuration -> a token is a 64-bit integer ID used to identify each partition

* A node claims ownership of the range of values less than or equal to each token and greater than the last token of the previous node, known as a token range.

image::rings.png[Rings]

* Data is assigned to nodes by using a *hash function to calculate a token for the partition key*. This partition key token is compared to the token values for the various nodes to identify the range, and therefore the node, that owns the data.

CQL language provides a `token()` function - request the value of the token corresponding to a partition key

----
SELECT last_name, first_name, token(last_name) FROM user;

----

== Virtual Nodes

Instead of assigning a single token to a node, the token range is broken up into multiple smaller ranges.

* Each physical node is then assigned multiple tokens.
* each node has been assigned 256 of these tokens, meaning that it represents 256 virtual nodes
* number of vnodes by setting the`num_tokens` property in the cassandra.yaml

== Partitioners

A partitioner determines how data is distributed across the nodes in the cluster.

* organizes rows in partitions.
* Each row has a partition key
* A partitioner is a hash function for computing the token of a partition key.
* compute the token based on the partition key columns

* Any clustering columns that may be present in the primary key are used to determine the ordering of rows within a given node

The Murmur3Partitioner was added in 1.2 and has been the default partitioner since then

image::partitioner.png[partitioner]

== Replication Strategies

A node serves as a replica for different ranges of data.

* replication factor is the number of nodes in your cluster that will receive copies (replicas) of the same data

The first replica will always be the node that claims the range in which the token falls, but the remainder of the replicas are placed according to the replication strategy

*SimpleStrategy* places replicas at consecutive nodes around the ring, starting with the node indicated by the partitioner

*NetworkTopologyStrategy* allows you to specify a different replication factor for each data center. Within a data center, it allocates replicas to different racks in order to maximize availability.

== Consistency Levels

* For *read queries* -> the consistency level specifies how many replica nodes must respond to a read request before returning the data.
* For *write operations* -> the consistency level specifies how many replica nodes must respond for the write to be reported as successful to the client.

Consistency levels:

* *ONE, TWO, and THREE* - an absolute number of replica nodes that must respond to a request

* *QUORUM* - requires a response from a majority of the replica nodes

Q=floor(replication factor/2+1)

RF=3 -> Q=2, RF=4 -> Q=3, RF=5 -> Q=3

* *ALL* - requires a response from all of the replicas

The replication factor is set per keyspace. The consistency level is specified per query, by the client.

R(read replica count) + W(write replica count) > RF = strong consistency

_Recommended way to achieve strong consistency in Cassandra is to write and read using the QUORUM or LOCAL_QUORUM consistency levels_

== Queries and Coordinator Nodes

image::coordinatorNodes.png[Coordinator Nodes]

A client may connect to any node in the cluster to initiate a read or write query -> This becomes *the coordinator node*.

* The coordinator identifies which nodes are replicas for the data that is being written or read and forwards the queries to them.

* The coordinator node contacts all replicas, as determined by the consistency level and replication factor

== Hinted Handoff

A write request is sent to Cassandra, but a replica node where the write properly belongs is not available due to network partition, hardware failure, or some other reason.

* If the replica node where the write belongs has failed, the coordinator will create a hint, which is a small reminder
* once it detects via gossip that node B is back online, node A will “hand off” to node B the “hint” regarding the write.

Cassandra holds a separate hint for each partition that is to be written.

* In general, hints do not count as writes for the purposes of consistency level.
* The exception is the consistency level ANY - a hinted handoff alone will count as sufficient toward the success of a write operation.
 ** The write is considered durable, but the data may not be readable until the hint is delivered to the target replica.


When the other nodes notice that the failed node has come back online, they tend to flood that node with requests -> Cassandra limits the storage of hints to a configurable time window. It is also possible to disable hinted handoff entirely.

== Anti-Entropy, Repair, and Merkle Trees

Anti-entropy protocols are a type of gossip protocol for repairing replicated data.

Modes replica synchronization: *read repair and anti-entropy repair*.

Read repair:

* refers to the synchronization of replicas as data is read.
* Cassandra reads data from multiple replicas in order to achieve the requested consistency level, and detects if any replicas have out-of-date values.
* If an insufficient number of nodes have the latest value, a read repair is performed immediately to update the out-of-date replicas.
* Otherwise, the repairs can be performed in the background after the read returns.

Anti-entropy repair (manual repair):

* is a manually initiated operation performed on nodes as part of a regular maintenance process.
* is executed by using a tool called nodetool
* Running nodetool repair causes Cassandra to execute a *validation compaction*
* During a validation compaction, the server initiates a TreeRequest/TreeReponse conversation to exchange *Merkle trees* with neighboring replicas

_The Merkle tree is a hash representing the data in that table._

* If the trees from the different nodes don’t match, they have to be reconciled (or “repaired”) to determine the latest data values they should all be set to.

[NOTE]
====
Each table has its own Merkle tree. +
The tree is created as a snapshot during a validation compaction. +
It is kept only as long as is required to send it to the neighboring nodes on the ring
====

== Lightweight Transactions and Paxos

*Linearizable consistency* - to guarantee that no other client can come in between our read and write queries with their own modification

Cassandra supports a *lightweight transaction (LWT)* mechanism that provides linearizable consistency.

Cassandra’s LWT implementation is based on Paxos.

=== Paxos

*Paxos is a consensus algorithm* that allows distributed peer nodes to agree on a proposal, without requiring a leader to coordinate a transaction

Basic Paxos algorithm consists of two stages:

* prepare/promise
* propose/accept

To modify data:

* a coordinator node can propose a new value to the replica nodes, taking on the role of leader.

IMPORTANT:  Other nodes may act as leaders simultaneously for other modifications

* Each replica node checks the proposal, and if the proposal is the latest it has seen, it promises to not accept proposals associated with any prior proposals

* Each replica node also returns the last proposal it received that is still in progress.

* If the proposal is approved by a majority of replicas, the leader commits the proposal, but with the caveat that it must first commit any in-progress proposals that preceded its own proposal

The Cassandra implementation extends the basic Paxos algorithm to support the desired read-before-write semantics (also known as check-and-set), and to allow the state to be reset between transactions

* Prepare/Promise
* Read/Results
* Propose/Accept
* Commit/Ack

NOTE: A successful transaction requires four round-trips between the coordinator node and replicas

* Cassandra’s lightweight transactions are limited to a single partition

== Memtables, SSTables, and Commit Logs

Cassandra stores data both in memory and on disk to provide both high performance and durability.

image::storageEngine.png[Storage engine]

When a node receives a write operation -> *immediately* writes the data to a *commit log*

If the node crashes the *commit log gets replayed* (only time when the commit is read)

====
cqlsh> DESCRIBE KEYSPACE my_keyspace ;

CREATE KEYSPACE my_keyspace WITH replication =
{'class': 'SimpleStrategy',
'replication_factor': '1'}  AND durable_writes = true;
====

NOTE: Setting the durable_writes value to false increases the speed of writes, but also risks losing data if the node goes down before the data is flushed from memtables into SSTables.

After it’s written to the commit log -> the value is written to a memory-resident data structure called the *memtable*.

NOTE: Each memtable contains data for a specific table.

Starting with the 2.1 release have moved some memtable data to native memory. (memtables were stored on the JVM heap)

When the number of objects stored in the memtable reaches a threshol:

* *the contents of the memtable are flushed to disk in a file called an SSTable*(Sorted String Table).
* A new memtable is then created.
* This flushing is a nonblocking operation

NOTE: multiple memtables may exist for a single table, one current and the rest waiting to be flushed.

Each commit log maintains an *internal bit flag to indicate whether it needs flushing*.

* When a write operation is first received, it is written to the commit log and its bit flag is set to 1.

* There is only one bit flag per table, because only one commit log is ever being written to across the entire server.

* All writes to all tables will go into the same commit log, so the bit flag indicates whether a particular commit log contains anything that hasn’t been flushed for a particular table.

* Once the memtable has been properly flushed to disk, the corresponding commit log’s bit flag is set to 0, indicating that the commit log no longer has to maintain that data for durability purposes.

NOTE: commit logs have a configurable rollover threshold, and once this file size threshold is reached, the log will roll over, carrying with it any surviving dirty bit flags.

IMPORTANT: Once a memtable is flushed to disk as an SSTable, it is immutable and cannot be changed by the application

SSTables compaction changes only their on-disk representation. (merge step of mergesort)

SSTables Compression:

* since the 1.0 release
* configurable per table

NOTE: All writes are sequential. The write operation is just an immediate append, and then compaction helps to organize for better future read performance.

== Bloom Filters

* used to boost the performance of reads

* are nondeterministic because it is possible to get a false-positive read from a Bloom filter, but not a false-negative

* mapping the values in a data set into a bit array and condensing a larger data set into a digest string using a hash function

* The filters are stored in memory and are used to improve performance by reducing the need for disk access on key lookups

IMPORTANT: Cassandra maintains a Bloom filter for each SSTable. When a query is performed, the Bloom filter is checked first before accessing disk. Because false-negatives are not possible, if the filter indicates that the element does not exist in the set, it certainly doesn’t; but if the filter thinks that the element is in the set, the disk is accessed to make sure.


Steps:

_The bloom filter essentially consists of a bit vector of length m_

* To add an item to the bloom filter, we feed it to k different hash functions and set the bits at the resulting positions.

** Note that sometimes the hash functions produce overlapping positions, so less than k positions may be set.

* To test if an item is in the filter, again we feed it to the k hash functions. This time, we check to see if any of the bits at these positions are not set. If any are not set, it means the item is definitely not in the set. Otherwise, it is probably in the set.

Source: https://www.jasondavies.com/bloomfilter/[Bloom filter]

== Caching

* The *key cache* stores a map of partition keys to row index entries, facilitating faster read access into SSTables stored on disk. The key cache is stored on the JVM heap.

* The *counter cache* was added in the 2.1 release to improve counter performance by reducing lock contention for the most frequently accessed counters.


* The *row cache*  caches entire rows and can greatly speed up read access for frequently accessed rows, at the cost of more memory usage. The row cache is stored in off-heap memory.

* The *chunk cache* was added in the 3.6 release to store uncompressed chunks of data read from SSTable files that are accessed frequently. The chunk cache is stored in off-heap memory.

By default, key and counter caching are enabled, while row caching is disabled, as it requires more memory.

Cassandra saves its caches to disk periodically in order to warm them up more quickly on a node restart.

== Compaction

During compaction, the data in SSTables is merged: the keys are merged, columns are combined, obsolete values are discarded, and a new index is created.

IMPORTANT: each SSTable consists of multiple files, including *Data, Index, and Filter*

The merged data is sorted, a new index is created over the sorted data, and the freshly merged, sorted, and indexed data is written to a single new SSTable

NOTE: The compaction strategy is an option that is set for each table

Compaction strategies:

* SizeTieredCompactionStrategy (STCS) is the default compaction strategy and is recommended for write-intensive tables.

* LeveledCompactionStrategy (LCS) is recommended for read-intensive tables.

* TimeWindowCompactionStrategy (TWCS) is intended for time series or otherwise date-based data.

== Deletion and Tombstones

To prevent deleted data from being reintroduced, Cassandra uses a concept called a tombstone.

*A tombstone* is a marker that is kept to indicate data that has been deleted.

When you execute a delete operation:

* the data is not immediately deleted.
* Instead, it’s treated as an update operation that places a tombstone on the value.

Tombstones are not kept forever.

* instead, they are removed as part of compaction.
* a setting per table called *gc_grace_seconds (Garbage Collection Grace Seconds)* -   the amount of time that nodes will wait to garbage collect (or compact) tombstones.
* By default, it’s set to 864,000 seconds, the equivalent of 10 days.

== Managers and Services

Basic internal control mechanisms:

=== Cassandra Daemon

`org.apache.cassandra.service.CassandraDaemon`

* the life cycle of the Cassandra service running on a single node
* typical life cycle operations that you might expect: start, stop, activate, deactivate, and destroy.

* to create an in-memory Cassandra instance programmatically by using the class `org.apache.cassandra.service.EmbeddedCassandraService`

=== Storage Engine

* classes in the `org.apache.cassandra.db` package

* `ColumnFamilyStore` class, which manages all aspects of table storage, including commit logs, memtables, SSTables, and indexes

=== Storage Service

Cassandra wraps the storage engine with a service that is represented by the `org.apache.cassandra.service.StorageService` class.

* contains the node’s token, which is a marker indicating the range of data that the node is responsible for.

* The server starts up with a call to the initServer method of this class, upon which the server *registers the thread pools* used to manage various tasks, makes some determinations about its state

=== Storage Proxy

The `org.apache.cassandra.service.StorageProxy` sits in front of the StorageService to handle the work of responding to client requests.

* It coordinates with other nodes to store and retrieve data, including storage of hints when needed.
* helps manage lightweight transaction processing.

=== Messaging Service

The purpose of `org.apache.cassandra.net.MessagingService` is to manage all inbound and outbound messages from this node to and from other nodes, except for SSTable streaming.

* Incoming messages are routed to the other services for handling.
* Outgoing messages may optionally have callbacks, which are invoked when a response is received from the other node.

=== Stream Manager

Streaming is Cassandra’s optimized way of sending SSTable files from one node to another via a *persistent TCP connection*

* all other communication between nodes occurs via serialized messages.

Streaming may occur:

* when tokens need to be reallocated across the cluster, such as when a node is added or removed.
* during repair processing or when a node is being replaced or rebuilt.

=== CQL Native Transport Server

The CQL Native Protocol is the *binary protocol used by clients to communicate with Cassandra*.

* The `org.apache.cassandra.transport` package contains the classes that implement this protocol, including the Server.

* This native transport server manages client connections and routes incoming requests, delegating the work of performing queries to the StorageProxy.

== System Keyspaces

* Cassandra makes use of its own storage to keep track of metadata about the cluster and local node

[source]
====
cqlsh> DESCRIBE TABLES;

Keyspace system_traces +
`----------------------` +
events  sessions

Keyspace system_schema +
`----------------------` +
tables     triggers    views    keyspaces  dropped_columns +
functions  aggregates  indexes  types      columns

Keyspace system_auth +
`--------------------` +
resource_role_permissons_index  network_permissions  role_permissions +
role_members                    roles

Keyspace system +
`---------------` +
repairs              view_builds_in_progress  paxos +
available_ranges     prepared_statements      size_estimates +
batches              peers                    built_views +
peer_events_v2       compaction_history       local +
available_ranges_v2  sstable_activity         transferred_ranges +
peers_v2             peer_events +
"IndexInfo"          transferred_ranges_v2

Keyspace system_distributed +
`---------------------------` +
repair_history  view_build_status  parent_repair_history
====

* Information about the structure of the cluster communicated via gossip is stored in `system.local` and `system.peers`. These tables hold information about the local node and other nodes in the cluster, including IP addresses, locations by data center and rack, token ranges, CQL, and protocol versions.

* The `system.transferred_ranges` and `system.available_ranges` track token ranges previously managed by each node and any ranges needing allocation.

* The construction of materialized views is tracked in the `system.view_builds_in_progress` and system.built_views tables, resulting in the views available in system_schema.views.

* User-provided extensions include system_schema.types for user-defined types, `system_schema.triggers` for triggers configured per table, `system_schema.functions` for user-defined functions, and `system_schema.aggregates` for user-defined aggregates.

* The `system.paxos` table stores the status of transactions in progress, while the `system.batches` table stores the status of batches.

* The `system.size_estimates` stores the estimated number of partitions per table and mean partition size.

* The cluster’s definitions of the keyspaces, tables, and indexes are stored by the `system_schema.keyspaces`, `system_schema.tables`, and `system_schema.columns`.

* The `system_traces` keyspace contains tables that store information about query traces

* The `system_auth` keyspace contains tables that store information about the users, roles, and permissions Cassandra uses to provide authentication and authorization features


