= Resillience patterns
:toc:

== Bulkhead

https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead[Bulkhead]

Elements of an application are isolated into pools so that if one fails, the others will continue to function.

=== Context and problem
A cloud-based application may include multiple services, with each service having one or more consumers. 
Excessive load or failure in a service will impact all consumers of the service.

Moreover, a consumer may send requests to multiple services simultaneously, using resources for each request. 
When the consumer sends a request to a service that is misconfigured or not responding, the resources used by the client's request may not be freed in a timely manner. 
As requests to the service continue, those resources may be exhausted. 

=== Solution
Partition service instances into different groups, based on consumer load and availability requirements. 
This design helps to isolate failures, and allows you to sustain service functionality for some consumers, even during a failure.

== Fault Tolerance

=== Context and Problem

The failure of an IT system can disrupt the customer journey in a way that the customer cannot continue with his desired activity. +
There is often no proper fault tolerance or graceful degradation so that even a small failure can cause total breakdown.

==== Causes:

* incorrect failed strategy - the consumer does not know how to act if a provider is unavailable
* no automatic recovery from failures - the system does not know how to recover from temporarily failing systems
* proovider is a Single point of Failure - provider does not have a redundant isolated instance

=== Solution

*Fault tolerance* is the property that enables a system to continue operating properly in the event of the failure of (or one or more faults within) some of its components. If its operating quality decreases at all, the decrease is proportional to the severity of the failure.

Means to implement fault tolerenant distributed systems:

* *Isolation* - to prevent that a system fails as a whole. 
  ** *bulkheads* to isolate compoonents
  ** *stovepipes* for chains
  
* *Health checking* - to discover failures and implement *fail-over to healthy redundant* isolated instances of the same component to prevent outage
* *Timeouts* - to prevent blocking actions caused by the failing system 
* *Retries* - to be resilient to transient failures - the provider needs to be idempotent to the retry
* *Circuit breaker* - to prevent cascading propagation of the failure and to allow a *fail-fast strategy*
* *Graceful degradation* - to offer the customer a limited set of functionality to prevent outage. 
* *Multiple different implementation* - in case of a failure of another implementation of a component - provide a fall-back mechanism (Cache, different channel)

== Stovepipe

=== Context and problem

Ability to recover from a disaster or major incident 

=== Solution 

Deploy multiple datacenters without any interdependecies in a stovepipe pattern. In a dual datacenter strategy this is necessary because one datacenter must be able to handle all capabilities (accept Disaster Recovery)

=== Recipe

Pattern implemnted with the recipe of datacenter twins.

== Timeout 

=== Context and problem

Ability to preserve responsiveness independent of downstream system's latency.

=== Solution 

Sockets API defines two types of timeouts:

* The *connection timeout* denotes the maximum time elapsed before the connection is established or an error occurs.
* The *socket timeout* defines the maximum period of inactivity between two consecutive data packets arriving on the client side after a connection has been established.

=== Retry

=== Context and problem

An application that communicates with elements running in the cloud must be sensitive to the transient faults that can occur in this environment. Such faults include the momentary loss of network connectivity to components and services, the temporary unavailability of a service, or timeouts that arise when a service is busy.

These faults are typically self-correcting, and if the action that triggered a fault is repeated after a suitable delay it is likely to be successful. 
