= Resillience patterns
:toc:

== Bulkhead

https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead[Bulkhead]

Elements of an application are isolated into pools so that if one fails, the others will continue to function.

=== Context and problem
A cloud-based application may include multiple services, with each service having one or more consumers. 
Excessive load or failure in a service will impact all consumers of the service.

Moreover, a consumer may send requests to multiple services simultaneously, using resources for each request. 
When the consumer sends a request to a service that is misconfigured or not responding, the resources used by the client's request may not be freed in a timely manner. 
As requests to the service continue, those resources may be exhausted. 

=== Solution
Partition service instances into different groups, based on consumer load and availability requirements. 
This design helps to isolate failures, and allows you to sustain service functionality for some consumers, even during a failure.

== Fault Tolerance

=== Context and Problem

The failure of an IT system can disrupt the customer journey in a way that the customer cannot continue with his desired activity. +
There is often no proper fault tolerance or graceful degradation so that even a small failure can cause total breakdown.

==== Causes:

* incorrect failed strategy - the consumer does not know how to act if a provider is unavailable
* no automatic recovery from failures - the system does not know how to recover from temporarily failing systems
* proovider is a Single point of Failure - provider does not have a redundant isolated instance

=== Solution

*Fault tolerance* is the property that enables a system to continue operating properly in the event of the failure of (or one or more faults within) some of its components. If its operating quality decreases at all, the decrease is proportional to the severity of the failure.

Means to implement fault tolerenant distributed systems:

* *Isolation* - to prevent that a system fails as a whole. 
  ** *bulkheads* to isolate compoonents
  ** *stovepipes* for chains
  
* *Health checking* - to discover failures and implement *fail-over to healthy redundant* isolated instances of the same component to prevent outage
* *Timeouts* - to prevent blocking actions caused by the failing system 
* *Retries* - to be resilient to transient failures - the provider needs to be idempotent to the retry
* *Circuit breaker* - to prevent cascading propagation of the failure and to allow a *fail-fast strategy*
* *Graceful degradation* - to offer the customer a limited set of functionality to prevent outage. 
* *Multiple different implementation* - in case of a failure of another implementation of a component - provide a fall-back mechanism (Cache, different channel)

== Stovepipe

=== Context and problem

Ability to recover from a disaster or major incident 

=== Solution 

Deploy multiple datacenters without any interdependecies in a stovepipe pattern. In a dual datacenter strategy this is necessary because one datacenter must be able to handle all capabilities (accept Disaster Recovery)

=== Recipe

Pattern implemnted with the recipe of datacenter twins.

== Timeout 

=== Context and problem

Ability to preserve responsiveness independent of downstream system's latency.

=== Solution 

Sockets API defines two types of timeouts:

* The *connection timeout* denotes the maximum time elapsed before the connection is established or an error occurs.
* The *socket timeout* defines the maximum period of inactivity between two consecutive data packets arriving on the client side after a connection has been established.

== Retry

Source: https://docs.microsoft.com/en-us/previous-versions/msp-n-p/dn589788(v=pandp.10)[Retry]

=== Context and problem

An application that communicates with elements running in the cloud must be sensitive to the transient faults that can occur in this environment. Such faults include the momentary loss of network connectivity to components and services, the temporary unavailability of a service, or timeouts that arise when a service is busy.

These faults are typically self-correcting, and if the action that triggered a fault is repeated after a suitable delay it is likely to be successful. 

=== Solution 

If an application detects a failure when it attempts to send a request to a remote service, it can handle the failure by using the following strategies:

* If the fault indicates that the *failure is not transient or is unlikely to be successful if repeated* (for example, an authentication failure caused by providing invalid credentials is unlikely to succeed no matter how many times it is attempted), the application should *abort* the operation and report a suitable exception.
* If the specific fault reported is *unusual or rare*, it may have been caused by freak circumstances such as a network packet becoming corrupted while it was being transmitted. In this case, the application *could retry* the failing request again immediately because the same failure is unlikely to be repeated and the request will probably be successful.
* If the fault is caused by one of the more *commonplace connectivity or “busy” failures*, the network or service may require a short period while the connectivity issues are rectified or the backlog of work is cleared. The application *should wait for a suitable time before retrying the request*.

For the more common transient failures, the period between retries should be chosen so as to spread requests from multiple instances of the application as evenly as possible.

If the request still fails, the application can wait for a further period and make another attempt. If necessary, this process can be repeated with increasing delays between retry attempts until some maximum number of requests have been attempted and failed. 

*The delay time can be increased incrementally*, or a timing strategy such as *exponential back-off* can be used, depending on the nature of the failure and the likelihood that it will be corrected during this time.

==== Netflix

The Netflix client allows you to create your own retry handler or use one of the predefined handlers such as
*DefaultLoadBalancerRetryHandler* or *RequestSpecificRetryHandler*. The latter allows for creating a retry handler for a
specific request. +
The retry handlers enable you, apart from enabling and disabling the mechanism, to set two important
values:

* MaxRetriesOnSameServer, the number of retries that should be attempted before switching to the next server in the
load-balanced pool (if available).
* MaxRetriesOnNextServer, the number of other servers that should be attempted before failing definitively.

The RetryHandler will determine (depending on the error) whether it makes sense to retry on the same server or move on
to the next. This behavior is determined by implementing the isRetriableException and isCircuitTrippingException methods.

----
LoadBalancerCommand.<String>builder()  
            .withRetryHandler(new DefaultLoadBalancerRetryHandler(q, 1, true))  
            // retry once, then try on 1 new server  
            .build();  
----

==== Ribbon

Using Ribbon's HttpResourceGroup (through ClientOptions):
----
HttpResourceGroup httpResourceGroup = Ribbon.createHttpResourceGroup("myClient",  
    ClientOptions.create()  
       .withMaxAutoRetries(0)  
       .withMaxAutoRetriesNextServer(1)  
       .withLoadBalancerEnabled(true)  
----

Hystrix (incorporated in Ribbon) has something called a FallbackHandler. It enables you to define behavior for when
all retry attempts (if any) have failed. You could introduce an alternative means of achieving the same goal, supply
a friendly message, degrade gracefully

----
httpResourceGroup.newTemplateBuilder("myTemplate", ByteBuf.class)  
    .withFallbackProvider(new MyFallbackHandler())  
----

==== Finagle

Using Finagle we can enable retry on any service by stacking a Retry filter on the existing service. This can be done in
the same way as adding a timeout filter.

----
val policy           = new ConservativeHttpRetryPolicy()  
val retryFilter      = new RetryFilter(policy)  
val serviceWithRetry = retryFilter andThen service  
----


== Service Commands Patterns

(Connectivity Patterns)

=== Context and Problem

Implementing services that provide Command interfaces (Transactions, Create, Update, Deletes) can be performed with diffferent patterns.

=== Solution

Implement service queries only by using one of these patterns:

* Oneway
* Oneway + Notification
* Request-Reply
* Notification
* Notification + Oneway

|===
|Service integration | One way | Request-reply | Notification

|Communication | async |sync | async

|Started by | requestor | requestor | provider

|Requestor/Consumer messages 
|out: one-way(req) / in: notification (res)
|out: req / in: res
|in: notification(req) / out: one way (res)

|Provider messages 
|in: one way (req) / out: notification (res)
|in: req / out: res
|out: notification(req) / in: one way (res)


|===




